<div align="center">

# ğŸ“± Mobile AI Apps - iOS & Android

### ğŸš€ Ultra-Modern Mobile AI Development Resources

<p align="center">
  <img src="https://img.shields.io/badge/Platform-iOS%20%7C%20Android-blue?style=for-the-badge&logo=apple&logoColor=white" alt="Platform">
  <img src="https://img.shields.io/badge/AI-SOTA%202024--2025-orange?style=for-the-badge&logo=tensorflow&logoColor=white" alt="AI">
  <img src="https://img.shields.io/badge/Status-Active-success?style=for-the-badge" alt="Status">
</p>

<p align="center">
  <strong>ğŸ¯ State-of-the-Art AI Models | ğŸ”¥ Latest Trends | âš¡ High Performance</strong>
</p>

</div>

---

## ğŸ“‘ Table of Contents

- [ğŸ¨ No Code / Low Code Platforms](#-no-code--low-code-platforms)
- [ğŸ iOS Development](#-ios-development)
- [ğŸ¤– Android Development](#-android-development)
- [ğŸ”¥ SOTA 2024-2025: Vision Models](#-sota-2024-2025-vision-models)
- [ğŸ§  SOTA 2024-2025: Mobile LLMs & Multimodal](#-sota-2024-2025-mobile-llms--multimodal)
- [âš¡ Efficient Mobile Models](#-efficient-mobile-models)
- [ğŸ¯ Object Detection & Segmentation](#-object-detection--segmentation)
- [ğŸ“¦ Deployment & Optimization](#-deployment--optimization)
- [ğŸ› ï¸ Tools & Utilities](#ï¸-tools--utilities)

---

## ğŸ¨ No Code / Low Code Platforms

Build powerful mobile apps without extensive coding!

| Platform | Description | Best For |
|----------|-------------|----------|
| ğŸ”· [AppSheet](https://about.appsheet.com/home/) | Google's no-code platform | Data-driven apps, Quick prototypes |
| âš¡ [PowerApps](https://powerapps.microsoft.com/en-gb/) | Microsoft's low-code solution | Enterprise apps, Business workflows |
| ğŸ¯ [FlutterFlow](https://flutterflow.io/) | Visual Flutter builder | Production-ready mobile apps |
| ğŸš€ [Adalo](https://www.adalo.com/) | No-code app builder | MVP, Startups |

---

## ğŸ iOS Development

### ğŸ“š Essential Resources

- ğŸ“ [The Complete iOS App Development Bootcamp](https://www.appbrewery.co/p/ios-course-resources/)
- ğŸ“± [Run Your App on iPhone or Simulator](https://www.youtube.com/watch?v=Fo1A36RsoCI)
- âš™ï¸ **Device Management**: iOS > Settings > General > VPN & Device Management

### ğŸ”§ Advanced Integration

- ğŸ”— [How to consume C++ code in Swift](https://anuragajwani.medium.com/how-to-consume-c-code-in-swift-b4d64a04e989)
- âš¡ [NCNN-Swift](https://github.com/zhuzilin/ncnn-swift) - High-performance neural network inference

### ğŸ“¦ iOS AI Frameworks

| Framework | Description | Stars |
|-----------|-------------|-------|
| [Core ML](https://developer.apple.com/machine-learning/core-ml/) | Apple's ML framework | Official |
| [TensorFlow Lite iOS](https://www.tensorflow.org/lite/guide/ios) | TFLite for iOS | ![Stars](https://img.shields.io/github/stars/tensorflow/tensorflow?style=social) |
| [PyTorch Mobile](https://pytorch.org/mobile/ios/) | PyTorch on iOS | ![Stars](https://img.shields.io/github/stars/pytorch/pytorch?style=social) |
| [ONNX Runtime iOS](https://onnxruntime.ai/docs/tutorials/mobile/) | Cross-platform inference | ![Stars](https://img.shields.io/github/stars/microsoft/onnxruntime?style=social) |

---

## ğŸ¤– Android Development

### ğŸ”§ Android AI Frameworks

| Framework | Description | Performance |
|-----------|-------------|-------------|
| [TensorFlow Lite](https://www.tensorflow.org/lite/android) | Google's mobile ML | âš¡âš¡âš¡âš¡âš¡ |
| [ML Kit](https://developers.google.com/ml-kit) | Ready-to-use ML APIs | âš¡âš¡âš¡âš¡ |
| [MediaPipe](https://developers.google.com/mediapipe) | ML pipelines | âš¡âš¡âš¡âš¡âš¡ |
| [NCNN](https://github.com/Tencent/ncnn) | Tencent's framework | âš¡âš¡âš¡âš¡âš¡ |
| [MNN](https://github.com/alibaba/MNN) | Alibaba's framework | âš¡âš¡âš¡âš¡âš¡ |

---

## ğŸ”¥ SOTA 2024-2025: Vision Models

### ğŸ¯ Object Detection - YOLO Series (Latest)

| Model | Year | Repository | Key Features |
|-------|------|------------|--------------|
| **YOLOv11** ğŸ†• | 2024 | [ultralytics/ultralytics](https://github.com/ultralytics/ultralytics) | Latest YOLO, improved accuracy & speed |
| **YOLOv10** | 2024 | [THU-MIG/yolov10](https://github.com/THU-MIG/yolov10) | NMS-free, real-time end-to-end |
| **YOLOv9** | 2024 | [WongKinYiu/yolov9](https://github.com/WongKinYiu/yolov9) | Programmable gradient information |
| **YOLO-World** ğŸŒ | 2024 | [AILab-CVC/YOLO-World](https://github.com/AILab-CVC/YOLO-World) | Open-vocabulary detection |
| **YOLOv8** | 2023 | [ultralytics/ultralytics](https://github.com/ultralytics/ultralytics) | Unified framework, SOTA performance |
| **YOLO-NAS** | 2023 | [Deci-AI/super-gradients](https://github.com/Deci-AI/super-gradients) | Neural Architecture Search |

### ğŸ­ Segmentation Models

| Model | Year | Repository | Innovation |
|-------|------|------------|------------|
| **SAM 2** ğŸ†• | 2024 | [facebookresearch/segment-anything-2](https://github.com/facebookresearch/segment-anything-2) | Video + Image segmentation |
| **Mobile-SAM** ğŸ“± | 2024 | [ChaoningZhang/MobileSAM](https://github.com/ChaoningZhang/MobileSAM) | 60x faster than SAM, mobile-friendly |
| **FastSAM** âš¡ | 2023 | [CASIA-IVA-Lab/FastSAM](https://github.com/CASIA-IVA-Lab/FastSAM) | 50x faster segmentation |
| **EfficientSAM** | 2024 | [yformer/EfficientSAM](https://github.com/yformer/EfficientSAM) | Lightweight SAM variant |
| **SAM-HQ** | 2023 | [SysCV/sam-hq](https://github.com/SysCV/sam-hq) | High-quality masks |

### ğŸ“ Polygon & Oriented Detection

| Model | Repository | Use Case |
|-------|------------|----------|
| **Yolo-ArbV2** | [Rhine-AI-Lab/Yolo-ArbV2](https://github.com/Rhine-AI-Lab/Yolo-ArbV2) | Arbitrary polygon detection |
| **Yolo-ArbPolygon** | [HRan2004/Yolo-ArbPolygon](https://github.com/HRan2004/Yolo-ArbPolygon) | Complex shapes |
| **Polygon-YOLOv5** | [LJT666666/polygon-yolov5](https://github.com/LJT666666/polygon-yolov5) | Polygon bounding boxes |
| **YOLOv3-Polygon** | [ming71/yolov3-polygon](https://github.com/ming71/yolov3-polygon) | Polygon annotations |

---

## ğŸ§  SOTA 2024-2025: Mobile LLMs & Multimodal

### ğŸ¤– On-Device Large Language Models

| Model | Year | Repository | Size | Key Features |
|-------|------|------------|------|--------------|
| **Gemma 2** ğŸ†• | 2024 | [google-deepmind/gemma](https://github.com/google-deepmind/gemma_pytorch) | 2B-27B | Google's open model, mobile-optimized |
| **Phi-3** ğŸ†• | 2024 | [microsoft/Phi-3](https://github.com/microsoft/Phi-3) | 3.8B | Microsoft's small language model |
| **Llama 3.2** ğŸ¦™ | 2024 | [meta-llama/llama3](https://github.com/meta-llama/llama-models) | 1B-3B | Meta's mobile-friendly LLM |
| **Qwen 2.5** | 2024 | [QwenLM/Qwen2.5](https://github.com/QwenLM/Qwen2.5) | 0.5B-72B | Alibaba's multilingual model |
| **SmolLM** | 2024 | [HuggingFaceTB/SmolLM](https://github.com/huggingface/smollm) | 135M-1.7B | Ultra-compact for edge devices |
| **MobileLLM** | 2024 | [facebookresearch/MobileLLM](https://github.com/facebookresearch/MobileLLM) | 125M-350M | Meta's mobile-first LLM |

### ğŸ‘ï¸ Vision-Language Models (Multimodal)

| Model | Year | Repository | Capabilities |
|-------|------|------------|--------------|
| **Florence-2** ğŸ†• | 2024 | [microsoft/Florence-2](https://huggingface.co/microsoft/Florence-2-large) | Unified vision tasks, prompting |
| **Qwen2-VL** ğŸ†• | 2024 | [QwenLM/Qwen2-VL](https://github.com/QwenLM/Qwen2-VL) | Multimodal understanding |
| **MobileVLM V2** ğŸ“± | 2024 | [Meituan-AutoML/MobileVLM](https://github.com/Meituan-AutoML/MobileVLM) | Mobile vision-language model |
| **LLaVA-Phi** | 2024 | [zhuyiche/llava-phi](https://github.com/zhuyiche/llava-phi) | Efficient multimodal |
| **MiniGPT-4** | 2023 | [Vision-CAIR/MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) | Vision-language understanding |
| **CogVLM2** | 2024 | [THUDM/CogVLM2](https://github.com/THUDM/CogVLM2) | Powerful visual understanding |

### ğŸ¨ Vision Models & Feature Extraction

| Model | Year | Repository | Innovation |
|-------|------|------------|------------|
| **DINOv2** | 2023 | [facebookresearch/dinov2](https://github.com/facebookresearch/dinov2) | Self-supervised vision features |
| **SAM-CLIP** | 2024 | [czczup/SAM-CLIP](https://github.com/czczup/SAM-CLIP) | SAM + CLIP integration |
| **MobileCLIP** | 2024 | [apple/ml-mobileclip](https://github.com/apple/ml-mobileclip) | Efficient CLIP for mobile |

---

## âš¡ Efficient Mobile Models

### ğŸš€ Backbone Networks

| Model | Year | Repository | Performance |
|-------|------|------------|-------------|
| **MobileNetV4** ğŸ†• | 2024 | [tensorflow/models](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet) | Universal mobile models |
| **EfficientViT** ğŸ†• | 2024 | [mit-han-lab/efficientvit](https://github.com/mit-han-lab/efficientvit) | MIT's efficient ViT |
| **FastViT** âš¡ | 2023 | [apple/ml-fastvit](https://github.com/apple/ml-fastvit) | Apple's fast vision transformer |
| **MobileViT v3** | 2023 | [apple/ml-cvnets](https://github.com/apple/ml-cvnets) | Hybrid CNN-Transformer |
| **MobileNetV3** | 2019 | [d-li14/mobilenetv2.pytorch](https://github.com/d-li14/mobilenetv2.pytorch) | Classic efficient model |
| **MobileNetV2** | 2018 | [kuan-wang/pytorch-mobilenet-v3](https://github.com/kuan-wang/pytorch-mobilenet-v3) | Lightweight backbone |
| **EdgeNeXt** | 2023 | [mmaaz60/EdgeNeXt](https://github.com/mmaaz60/EdgeNeXt) | Edge-optimized ConvNet |

### ğŸ¯ Efficient Object Detection

| Model | Repository | FPS (Mobile) |
|-------|------------|--------------|
| **YOLO-Lite** | [reu2018DL/YOLO-LITE](https://github.com/reu2018DL/YOLO-LITE) | 30+ |
| **NanoDet-Plus** | [RangiLyu/nanodet](https://github.com/RangiLyu/nanodet) | 25+ |
| **PP-PicoDet** | [PaddlePaddle/PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) | 30+ |
| **YOLO-FastestV2** | [dog-qiuqiu/Yolo-FastestV2](https://github.com/dog-qiuqiu/Yolo-FastestV2) | 40+ |

---

## ğŸ¯ Object Detection & Segmentation

### ğŸ“¦ Classic Detection Models

- ğŸ¯ [YOLOv5](https://github.com/ultralytics/yolov5/) - Popular, production-ready
- ğŸ“š [Ultralytics Docs](https://docs.ultralytics.com/) - Comprehensive documentation
- ğŸ”¬ [Roboflow YOLOv5 Tutorial](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb)

### ğŸ·ï¸ Annotation Tools & Converters

| Tool | Repository | Purpose |
|------|------------|---------|
| **JSON2YOLO** | [ultralytics/JSON2YOLO](https://github.com/ultralytics/JSON2YOLO) | Format conversion |
| **Imantics** | [jsbroks/imantics](https://github.com/jsbroks/imantics) | Mask to polygon conversion |
| **COCO API** | [cocodataset/cocoapi](https://github.com/cocodataset/cocoapi) | COCO format utilities |
| **Label Studio** | [heartexlabs/label-studio](https://github.com/heartexlabs/label-studio) | Modern annotation platform |
| **CVAT** | [opencv/cvat](https://github.com/opencv/cvat) | Computer vision annotation |

#### ğŸ“ Mask to Polygon Conversion

- ğŸ“– [Imantics Documentation](https://imantics.readthedocs.io/en/latest/api.html#polygons-object)
- ğŸ’¬ [COCO API Polygon Discussion](https://github.com/cocodataset/cocoapi/issues/39)

---

## ğŸ“¦ Deployment & Optimization

### ğŸš€ Mobile Inference Frameworks

| Framework | Platform | Year | Key Features |
|-----------|----------|------|--------------|
| **TensorFlow Lite** | iOS/Android | 2024 | ![TFLite](https://img.shields.io/badge/TFLite-FF6F00?style=flat&logo=tensorflow) |
| **Core ML** | iOS | 2024 | Native Apple ML |
| **ONNX Runtime Mobile** | iOS/Android | 2024 | Cross-platform |
| **MediaPipe** ğŸ†• | iOS/Android/Web | 2024 | Real-time ML pipelines |
| **NCNN** | iOS/Android | 2024 | Tencent, highly optimized |
| **MNN** | iOS/Android | 2024 | Alibaba's framework |
| **TNN** | iOS/Android | 2024 | Tencent's optimized |
| **Paddle Lite** | iOS/Android | 2024 | PaddlePaddle mobile |

### ğŸ”§ Model Optimization Tools

| Tool | Purpose | Repository |
|------|---------|------------|
| **ONNX** | Model exchange format | [onnx/onnx](https://github.com/onnx/onnx) |
| **TensorRT** | NVIDIA optimization | [NVIDIA/TensorRT](https://github.com/NVIDIA/TensorRT) |
| **OpenVINO** | Intel optimization | [openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino) |
| **Neural Compressor** | Intel quantization | [intel/neural-compressor](https://github.com/intel/neural-compressor) |

### ğŸ“Š Benchmarking

| Tool | Repository | Purpose |
|------|------------|---------|
| **AI Benchmark** | [AI-Benchmark/AI-Benchmark](https://ai-benchmark.com/) | Mobile AI performance |
| **MLPerf Mobile** | [mlcommons/mobile_app_open](https://github.com/mlcommons/mobile_app_open) | Standard benchmarks |
| **MobileNetV4 Benchmark** | [tensorflow/models](https://github.com/tensorflow/models) | Model comparison |

---

## ğŸ› ï¸ Tools & Utilities

### ğŸ¨ Pre-trained Model Hubs

- ğŸ¤— [Hugging Face](https://huggingface.co/models) - Largest ML model hub
- ğŸ”¥ [PyTorch Hub](https://pytorch.org/hub/) - PyTorch models
- ğŸ§  [TensorFlow Hub](https://www.tensorflow.org/hub) - TF models
- ğŸ [Core ML Models](https://developer.apple.com/machine-learning/models/) - Apple's model zoo
- ğŸ”¬ [ONNX Model Zoo](https://github.com/onnx/models) - ONNX format models

### ğŸ“š Learning Resources

- ğŸ“– [Made With ML](https://madewithml.com/) - ML production course
- ğŸ“ [Fast.ai](https://www.fast.ai/) - Practical deep learning
- ğŸš€ [Papers With Code](https://paperswithcode.com/) - Latest research + code
- ğŸ”¬ [Awesome Mobile AI](https://github.com/topics/mobile-ai) - Curated list

### ğŸ”¬ Research Papers (2024-2025)

| Paper | Topic | Link |
|-------|-------|------|
| YOLOv10 | Real-time Detection | [arXiv](https://arxiv.org/abs/2405.14458) |
| SAM 2 | Video Segmentation | [arXiv](https://arxiv.org/abs/2408.00714) |
| Gemma 2 | Language Models | [arXiv](https://arxiv.org/abs/2408.00118) |
| Florence-2 | Multimodal | [arXiv](https://arxiv.org/abs/2311.06242) |
| MobileNetV4 | Efficient CNNs | [arXiv](https://arxiv.org/abs/2404.10518) |

---

## ğŸ¯ Quick Start Examples

### ğŸ iOS - Core ML Example

```swift
import CoreML
import Vision

// Load YOLOv8 model
guard let model = try? VNCoreMLModel(for: YOLOv8().model) else { return }

// Create request
let request = VNCoreMLRequest(model: model) { request, error in
    guard let results = request.results as? [VNRecognizedObjectObservation] else { return }
    // Process results
}

// Perform inference
let handler = VNImageRequestHandler(cgImage: image)
try? handler.perform([request])
```

### ğŸ¤– Android - TFLite Example

```kotlin
// Load TFLite model
val interpreter = Interpreter(loadModelFile())

// Prepare input
val input = prepareInput(bitmap)
val output = Array(1) { FloatArray(1000) }

// Run inference
interpreter.run(input, output)

// Process results
val results = processOutput(output)
```

---

## ğŸ“Š Performance Comparison (2024)

| Model | Size | FPS (iOS) | FPS (Android) | mAP |
|-------|------|-----------|---------------|-----|
| YOLOv11n | 6MB | 45 | 40 | 39.5% |
| YOLOv10n | 5.8MB | 48 | 42 | 38.5% |
| Mobile-SAM | 40MB | 12 | 10 | 93.4% IoU |
| MobileNetV4-S | 3.8MB | 120 | 100 | 73.4% |
| Gemma-2B | 5GB | - | - | - |

---

## ğŸŒŸ Featured Projects

- ğŸ¯ [Awesome-Mobile-AI](https://github.com/yourusername/awesome-mobile-ai) - Curated awesome list
- ğŸ”¥ [Mobile-AI-Benchmark](https://github.com/yourusername/mobile-ai-benchmark) - Benchmarking suite
- ğŸ“± [AI-Camera-App](https://github.com/yourusername/ai-camera-app) - Production-ready app

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“„ License

This repository is for educational and reference purposes.

---

<div align="center">

### ğŸŒŸ Don't forget to star this repo if you find it helpful!

<p align="center">
  <img src="https://img.shields.io/badge/Made%20with-â¤ï¸-red?style=for-the-badge" alt="Made with love">
  <img src="https://img.shields.io/badge/AI-Powered-blue?style=for-the-badge&logo=openai" alt="AI Powered">
  <img src="https://img.shields.io/badge/Mobile-First-green?style=for-the-badge&logo=android" alt="Mobile First">
</p>

**ğŸš€ Updated for 2024-2025 | ğŸ“± Production-Ready | âš¡ High-Performance**

</div>
